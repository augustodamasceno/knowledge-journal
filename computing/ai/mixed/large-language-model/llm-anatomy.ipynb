{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f4a7762-1692-4f33-8382-bb01fbc1a5c3",
   "metadata": {},
   "source": [
    "<center><h1>Anatomy of a Large Language Model</h1></center>\n",
    "<center><i>Part of the Knowledge Journal Project</i></center>\n",
    "\n",
    "---\n",
    "\n",
    "**Author:** Augusto Damasceno  \n",
    "**Project Link:** [github.com/augustodamasceno/knowledge-journal](https://github.com/augustodamasceno/knowledge-journal)  \n",
    "**Last Updated:** August 19, 2025\n",
    "\n",
    "---\n",
    "<div style=\"text-align: right;\">\n",
    "<small>Copyright ¬© 2025, Augusto Damasceno</small><br>\n",
    "<small>SPDX-License-Identifier: <code>BSD-2-Clause</code></small>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a6b5b5-63c0-4633-a0c9-3172015a5f18",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "### **1. Definition**\n",
    "### **2. Chats and Engines**\n",
    "### **3. Data and Preprocessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2655cd7d-6749-4896-802b-d89425e13028",
   "metadata": {},
   "source": [
    "# Session 1 - Definition\n",
    "> All text within this session has been sourced from IBM [1].\n",
    "\n",
    "Large language models (LLMs) are a category of foundation models trained on immense amounts of data making them capable of understanding \n",
    "and generating natural language and other types of content to perform a wide range of tasks. \n",
    "\n",
    "\n",
    "## How large language models work\n",
    "\n",
    "LLMs operate by leveraging deep learning techniques and vast amounts of textual data. These models are typically based on a transformer architecture, like the generative pre-trained transformer, which excels at handling sequential data like text input. LLMs consist of multiple layers of neural networks, each with parameters that can be fine-tuned during training, which are enhanced further by a numerous layer known as the attention mechanism, which dials in on specific parts of data sets.\n",
    "\n",
    "During the training process, these models learn to predict the next word in a sentence based on the context provided by the preceding words. The model does this through attributing a probability score to the recurrence of words that have been tokenized, broken down into smaller sequences of characters. These tokens are then transformed into embeddings, which are numeric representations of this context.\n",
    "\n",
    "To ensure accuracy, this process involves training the LLM on a massive corpora of text (in the billions of pages), allowing it to learn grammar, semantics and conceptual relationships through zero-shot and self-supervised learning. Once trained on this training data, LLMs can generate text by autonomously predicting the next word based on the input they receive, and drawing on the patterns and knowledge they've acquired. The result is coherent and contextually relevant language generation that can be harnessed for a wide range of NLU and content generation tasks.\n",
    "\n",
    "Model performance can also be increased through prompt engineering, prompt-tuning, fine-tuning and other tactics like reinforcement learning with human feedback (RLHF) to remove the biases, hateful speech and factually incorrect answers known as ‚Äúhallucinations‚Äù that are often unwanted byproducts of training on so much unstructured data. This is one of the most important aspects of ensuring enterprise-grade LLMs are ready for use and do not expose organizations to unwanted liability, or cause damage to their reputation. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087b326b-d6b1-4fdb-b679-e953a74fbc5d",
   "metadata": {},
   "source": [
    "# Session 2 - Chats and Engines\n",
    "\n",
    "## Chats [2-24]\n",
    "| Chat Service üí¨ | Developer | Key Focus / Differentiator | Primary Model(s) | Notable Feature üöÄ |\n",
    "| :--- | :--- | :--- | :--- | :--- |\n",
    "| **Amazon Q** ‚òÅÔ∏è | Amazon AWS | Business-focused AI for DevOps and AWS cloud services | Proprietary (Titan) | Directly takes actions in your AWS environment |\n",
    "| **Character.AI** üé≠ | Character.AI | Character-driven, immersive roleplay conversations | Proprietary Models | User-generated characters and group chats |\n",
    "| **ChatGPT** ü§ñ | OpenAI | General-purpose leader with strong multimodal capabilities | GPT-4o | Native desktop app & advanced voice mode |\n",
    "| **Claude** üïäÔ∏è | Anthropic | Safety, nuanced instruction following, & industry-leading context | Claude 3.5 Sonnet | Massive 200k token context window |\n",
    "| **CLOVA X** üá∞üá∑ | Naver (Korea) | Korean-focused assistant, integrated with LINE | HyperCLOVA X | Dominant presence in the Korean and Japanese markets |\n",
    "| **Cohere Chat** üåç | Cohere | Multilingual enterprise AI with on-premises deployment options | Command R+, Command A | 256k-token window with 23-language support |\n",
    "| **Copilot (Github)** üíª | Microsoft | AI pair programmer for developers in the IDE | GPT-4-Turbo, GPT-4o | Real-time code completion and commands in VS Code |\n",
    "| **DeepSeek Chat** üêã | DeepSeek | Powerful open-weight models with massive context support | DeepSeek-V3 | 128k token context for free, strong coding focus |\n",
    "| **Ernie Bot** üêâ | Baidu (China) | Chinese market leader with search & cloud integration | ERNIE 4.0 Turbo | Deep integration with Baidu's ecosystem |\n",
    "| **Gemini** üåê | Google | Deep integration with Google ecosystem & services | Gemini 1.5 Pro/Flash | \"Google it\" button for fact-checking responses |\n",
    "| **Grok** ü§ñ | xAI | Real-time knowledge via integration with X (Twitter) platform | Grok-2 | \"Grok mode\" for unfiltered, sarcastic responses |\n",
    "| **HuggingChat** ü§ó | Hugging Face | Open-source AI hub for customizable, community-driven chat | Mixtral, Llama-based models | Access to thousands of community-hosted models |\n",
    "| **Kimi Chat** üìÑ | Moonshot AI (China) | Ultra-long context processing for document analysis | Moonshot AI Models | Supports up to 2 million characters of context |\n",
    "| **Le Chat (Mistral)** üêì | Mistral AI | High-performance models with strong multilingual support | Mistral Large 2, Codestral | Strong coding and European language capabilities |\n",
    "| **Meta AI** ü¶ô | Meta | Social-first AI integrated into WhatsApp, Instagram, Facebook | Llama 3 | On-the-go access via Ray-Ban Meta smart glasses |\n",
    "| **Microsoft Copilot** ü™Ñ | Microsoft | Deeply embedded in Windows, Edge, and Office 365 | GPT-4-Turbo, GPT-4o | Accessible via a dedicated keyboard key on Windows PCs |\n",
    "| **Perplexity** üîé | Perplexity AI | Answer engine with real-time web search and citations | Mixture of Experts (Proprietary + OpenAI) | Copilot mode for guided, interactive research |\n",
    "| **Pi** ü§ù | Inflection AI | Empathetic and supportive personal AI companion | Inflection-2 | Proactive, kind, and conversationally fluid |\n",
    "| **Qwen Chat** ü¶ö | Alibaba Cloud | High-performance multilingual AI for enterprise and research | Qwen3 | Excels in over 100 languages, strong reasoning benchmarks |\n",
    "| **Replika** ‚ù§Ô∏è | Luka, Inc. | AI companion with emotional intelligence and memory | Proprietary Models | AR avatar for immersive interaction |\n",
    "| **SenseNova** üåå | SenseTime (China)| Enterprise and industry-specific AI solutions | SenseNova 5.0 | Strong computer vision and image gen capabilities |\n",
    "| **Spark (Xinghuo)** ‚ú® | iFlytek (China) | Conversational AI with education & business focus | SparkDesk / iFlytek Models | Specialized hardware and software for education |\n",
    "| **YouChat** üîó | You.com | Search-integrated chatbot with citations and apps | Proprietary + OpenAI | \"YouApps\" for adding tools like image gen to the chat |\n",
    "> This table was generated with contributions from multiple large language models, including Gemini, ChatGPT, DeepSeek, and Grok.\n",
    "\n",
    "## Engines [25-42]\n",
    "| Engine ‚öôÔ∏è             | Developer / Org          | Key Focus / Differentiator               | Main Model(s)                            |\n",
    "| ---------------------- | ------------------------ | ------------------------------------------ | ---------------------------------------- |\n",
    "| **GPT-4 / GPT-4o** ü§ñ  | OpenAI                   | General-purpose, industry-leading, multimodal | GPT-4, GPT-4o                            |\n",
    "| **Gemini** üåê          | Google (DeepMind)        | Multimodal, deep Google ecosystem integration | Gemini 1.5 Pro, Ultra                    |\n",
    "| **Claude** üïäÔ∏è           | Anthropic                | Safety, constitutional AI, long context      | Claude 3 & 3.5 family (Opus, Sonnet)     |\n",
    "| **LLaMA 3.1** ü¶ô       | Meta                     | Open-weight, research + industry adoption    | LLaMA 3.1 (8B, 70B, 405B)                |\n",
    "| **Command** üìè         | Cohere                   | Enterprise, RAG, multilingual              | Command R+, Command R                    |\n",
    "| **Mistral** ü¶∏         | Mistral AI               | High-performance, open-source & MoE          | Mistral Large 2, Mixtral 8x22B           |\n",
    "| **Grok-2** üê¶‚Äç‚¨õ          | xAI                      | Social/chat integration, reasoning         | Grok-2                                   |\n",
    "| **DeepSeek-V3** üïµÔ∏è      | DeepSeek AI              | MoE efficiency, cost-effective scaling     | DeepSeek-V3 (671B total, 37B active)     |\n",
    "| **Qwen-3** üêâ          | Alibaba Cloud            | Strong MoE scaling, Chinese/English        | Qwen 3 family                            |\n",
    "| **Kimi K2** üõ∞Ô∏è          | Moonshot AI              | Agentic MoE, top coding/math performance     | Kimi K2 (1T total, 32B active)           |\n",
    "| **Jurassic-2** ü¶ñ      | AI21 Labs                | Enterprise NLP, writing assistance         | Jurassic-2 family                        |\n",
    "| **Amazon Titan** üè≠    | Amazon Web Services      | AWS ecosystem, enterprise applications     | Titan Text, Titan Multimodal             |\n",
    "| **ERNIE Bot** üßß       | Baidu                    | Chinese ecosystem, multimodal              | ERNIE 4.0                                |\n",
    "| **Phi-3** üß†           | Microsoft Research       | Small Language Models (SLMs), on-device AI | Phi-3 family (Mini, Small, Medium)       |\n",
    "| **Falcon** ü¶Ö          | TII (UAE)                | Multilingual, open-weight Apache 2.0 license | Falcon 40B, Falcon 180B                  |\n",
    "| **Yi-1.5** üèØ          | 01.AI                    | Open-weight, strong reasoning & multilingual | Yi-1.5 (6B, 9B, 34B, 200B)               |\n",
    "| **GLM-4** üìö          | Zhipu AI & Tsinghua University | Academic & enterprise focus, bilingual     | GLM-4 family                             |\n",
    "| **EleutherAI** ‚è≥      | EleutherAI (community)   | Open-source research, data transparency    | GPT-J, GPT-NeoX, Pythia                  |\n",
    "> This table was generated with contributions from multiple large language models, including Gemini, ChatGPT, DeepSeek, and Grok."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9fe471-80cc-47c6-a91c-ddab5360ab40",
   "metadata": {},
   "source": [
    "# Session 3 - Data and Preprocessing  \n",
    "\n",
    "> The data used in this notebook is the book Alice's Adventures in Wonderland by Lewis Carroll [44],  \n",
    "> available for download from Project Gutenberg [43]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b40a7df-e1be-49a5-bdad-6f06750de71c",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "68df5e47-c874-4b46-a0e0-bd168f67d83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "\n",
    "import pandas as pd\n",
    "import contractions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6be9e53-6560-41ab-a72b-37f8b1a6c571",
   "metadata": {},
   "source": [
    "## Cleaning Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4ecc5e07-20db-4d5f-8b97-b83070c03470",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_square_brackets(text: str) -> str:\n",
    "    return re.sub(r'\\[.*?\\]', '', text)\n",
    "    \n",
    "def clean_gutenberg_text(raw_text: str) -> str:\n",
    "    start_markers = [\n",
    "        r\"\\*\\*\\* START OF (THIS|THE) PROJECT GUTENBERG EBOOK .* \\*\\*\\*\",\n",
    "        r\"\\*\\*\\*START OF THE PROJECT GUTENBERG EBOOK.*\",\n",
    "    ]\n",
    "    end_markers = [\n",
    "        r\"\\*\\*\\* END OF (THIS|THE) PROJECT GUTENBERG EBOOK .* \\*\\*\\*\",\n",
    "        r\"End of (the|this) Project Gutenberg EBook\",\n",
    "        r\"End of Project Gutenberg's\",\n",
    "    ]\n",
    "\n",
    "    start_index = -1\n",
    "    for marker in start_markers:\n",
    "        match = re.search(marker, raw_text, re.IGNORECASE)\n",
    "        if match:\n",
    "            start_index = match.end()\n",
    "            break\n",
    "\n",
    "    if start_index == -1:\n",
    "        start_index = 0\n",
    "\n",
    "    end_index = -1\n",
    "    for marker in end_markers:\n",
    "        match = re.search(marker, raw_text, re.IGNORECASE)\n",
    "        if match:\n",
    "            end_index = match.start()\n",
    "            break\n",
    "            \n",
    "    if end_index == -1:\n",
    "        end_index = len(raw_text)\n",
    "\n",
    "    core_content = raw_text[start_index:end_index].strip()\n",
    "    cleaned_text = re.sub(r'\\n\\s*\\n', '\\n\\n', core_content)\n",
    "    cleaned_text = re.sub(r'(?<!\\n)\\n(?!\\n)', ' ', cleaned_text)\n",
    "    cleaned_text = cleaned_text.strip()\n",
    "\n",
    "    return cleaned_text\n",
    "\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    text_clean_gutenberg = clean_gutenberg_text(text)\n",
    "    output = remove_square_brackets(text_clean_gutenberg)\n",
    "    return output\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "802660e3-f1d8-4b0e-bd68-dee80b51b056",
   "metadata": {},
   "source": [
    "# Normalization Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "db195b91-2dbc-4c4c-aa45-0caaa89de972",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(text):\n",
    "    text_lower = text.lower()\n",
    "    text_normalized = unicodedata.normalize('NFKC', text_lower)\n",
    "    text_expanded_contractions = contractions.fix(text_normalized)\n",
    "    return text_expanded_contractions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac317df3-757d-44e2-a0d6-d8b161de3a6e",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a297671f-7339-467d-b120-885046fb11dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "book_filename = \"dataset/book1.txt\"\n",
    "with open(book_filename, 'r', encoding='utf-8') as file:\n",
    "    raw_data= file.read()\n",
    "\n",
    "data_clean = clean_text(raw_data)\n",
    "data = normalize_text(data_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65abd630-b020-400f-8a75-2367b47fa2df",
   "metadata": {},
   "source": [
    "# Comparison Raw and Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "548c5184-8865-4237-a9f9-dce804611a91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nalice‚Äôs adventures in wonderland\\n\\nby lewis carroll\\n\\nthe millennium fulcrum edition 3.0\\n\\ncontents\\n\\n chapter i.     down the rabbit-hole  chapter ii.    the pool of tears  chapter iii.   a caucus-race and a long tale  chapter iv.    the rabbit sends in a little bill  chapter v.     advice from a caterpillar  chapter vi.    pig and pepper  chapter vii.   a mad tea-party  chapter viii.  the queen‚Äôs croquet-ground  chapter ix.    the mock turtle‚Äôs story  chapter x.     the lobster quadrille  chapter xi.    who stole the tarts?  chapter xii.   alice‚Äôs evidence\\n\\nchapter i. down the rabbit-hole\\n\\nalice was beginning to get very tired of sitting by her sister on the bank, and of having nothing to do: once or twice she had peeped into the book her sister was reading, but it had no pictures or conversations in it, ‚Äúand what is the use of a book,‚Äù thought alice ‚Äúwithout pictures or conversations?‚Äù\\n\\nso she was considering in her own mind (as well as she could, for the hot day made her feel very sleepy and stupid), whether the pleasure of making a daisy-chain would be worth the trouble of getting up and picking the daisies, when suddenly a white rabbit with pink eyes ran close by her.\\n\\nthere was nothing so _very_ remarkable in that; nor did alice think it so _very_ much out of the way to hear the rabbit say to itself, ‚Äúoh dear! oh dear! i shall be late!‚Äù (when she thought it over afterwards, it occurred to her that she ought to have wondered at this, but at the time it all seemed quite natural); but when the rabbit actually _took a watch out of its waistcoat-pocket_, and looked at it, and then hurried on, alice started to her feet, for it flashed across her mind that she had never before seen a rabbit with either a waistcoat-pocket, or a watch to take out of it, and burning with curiosity, she ran across the field after it, and fortunately was just in time to see it pop down a large rabbit-hole under the hedge.\\n\\nin another moment down went alice after it, never once considering how in the world she was to get out again.\\n\\nthe rabbit-hole went straight on like a tunnel for some way, and then dipped suddenly down, so suddenly that alice had not a moment to think about stopping herself before she found herself falling down a very deep well.\\n\\neither the well was very deep, or she fell very slowly, for she had plenty of time as she went down to look about her and to wonder what was going to happen next. first, she tried to look down and make out what she was coming to, but it was too dark to see anything; then she looked at the sides of the well, and noticed that they were filled with cupboards and book-shelves; here and there she saw maps and pictures hung upon pegs. she took down a jar from one of the shelves as she passed; it was labelled ‚Äúorange marmalade‚Äù, but to her great disappointment it was empty: she did not like to drop the jar for fear of killing somebody underneath, so managed to put it into one of the cupboards as she fell past it.\\n\\n‚Äúwell!‚Äù thought al'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:3000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "953990cf-028e-464c-9635-2b24c0186a9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ufeffThe Project Gutenberg eBook of Alice's Adventures in Wonderland\\n    \\nThis ebook is for the use of anyone anywhere in the United States and\\nmost other parts of the world at no cost and with almost no restrictions\\nwhatsoever. You may copy it, give it away or re-use it under the terms\\nof the Project Gutenberg License included with this ebook or online\\nat www.gutenberg.org. If you are not located in the United States,\\nyou will have to check the laws of the country where you are located\\nbefore using this eBook.\\n\\nTitle: Alice's Adventures in Wonderland\\n\\nAuthor: Lewis Carroll\\n\\nRelease date: June 27, 2008 [eBook #11]\\n                Most recently updated: June 26, 2025\\n\\nLanguage: English\\n\\nCredits: Arthur DiBianca and David Widger\\n\\n\\n*** START OF THE PROJECT GUTENBERG EBOOK ALICE'S ADVENTURES IN WONDERLAND ***\\n\\n[Illustration]\\n\\n\\n\\n\\nAlice‚Äôs Adventures in Wonderland\\n\\nby Lewis Carroll\\n\\nTHE MILLENNIUM FULCRUM EDITION 3.0\\n\\nContents\\n\\n CHAPTER I.     Down the Rabbit-Hole\\n CHAPTER II.    The Pool of Tears\\n CHAPTER III.   A Caucus-Race and a Long Tale\\n CHAPTER IV.    The Rabbit Sends in a Little Bill\\n CHAPTER V.     Advice from a Caterpillar\\n CHAPTER VI.    Pig and Pepper\\n CHAPTER VII.   A Mad Tea-Party\\n CHAPTER VIII.  The Queen‚Äôs Croquet-Ground\\n CHAPTER IX.    The Mock Turtle‚Äôs Story\\n CHAPTER X.     The Lobster Quadrille\\n CHAPTER XI.    Who Stole the Tarts?\\n CHAPTER XII.   Alice‚Äôs Evidence\\n\\n\\n\\n\\nCHAPTER I.\\nDown the Rabbit-Hole\\n\\n\\nAlice was beginning to get very tired of sitting by her sister on the\\nbank, and of having nothing to do: once or twice she had peeped into\\nthe book her sister was reading, but it had no pictures or\\nconversations in it, ‚Äúand what is the use of a book,‚Äù thought Alice\\n‚Äúwithout pictures or conversations?‚Äù\\n\\nSo she was considering in her own mind (as well as she could, for the\\nhot day made her feel very sleepy and stupid), whether the pleasure of\\nmaking a daisy-chain would be worth the trouble of getting up and\\npicking the daisies, when suddenly a White Rabbit with pink eyes ran\\nclose by her.\\n\\nThere was nothing so _very_ remarkable in that; nor did Alice think it\\nso _very_ much out of the way to hear the Rabbit say to itself, ‚ÄúOh\\ndear! Oh dear! I shall be late!‚Äù (when she thought it over afterwards,\\nit occurred to her that she ought to have wondered at this, but at the\\ntime it all seemed quite natural); but when the Rabbit actually _took a\\nwatch out of its waistcoat-pocket_, and looked at it, and then hurried\\non, Alice started to her feet, for it flashed across her mind that she\\nhad never before seen a rabbit with either a waistcoat-pocket, or a\\nwatch to take out of it, and burning with curiosity, she ran across the\\nfield after it, and fortunately was just in time to see it pop down a\\nlarge rabbit-hole under the hedge.\\n\\nIn another moment down went Alice after it, never once considering how\\nin the world she was to get out again.\\n\\nThe rabbit-hole went straight on like a tunnel for some way, and then\\ndipped suddenly down, so suddenly that Alice had no\""
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data[:3000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9067b02e-2729-42ac-ab70-bdf5641fad75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "39adbe20-6541-4efc-b2e9-d906b63ee07d",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "[1] ‚ÄúWhat are large language models (LLMs)?,‚Äù IBM Think, [Online]. Available: https://www.ibm.com/think/topics/large-language-models. [Accessed: Aug. 19, 2025].\n",
    "\n",
    "[2] Amazon Web Services, Amazon Q. [Online]. Available: https://aws.amazon.com/q/. [Accessed: Aug. 19, 2025].\n",
    "\n",
    "[3] Character.AI, Character.AI. [Online]. Available: [suspicious link removed]. [Accessed: Aug. 19, 2025].\n",
    "\n",
    "[4] OpenAI, ChatGPT. [Online]. Available: https://chat.openai.com/. [Accessed: Aug. 19, 2025].\n",
    "\n",
    "[5] Anthropic, Claude. [Online]. Available: https://claude.ai/. [Accessed: Aug. 19, 2025].\n",
    "\n",
    "[6] Naver, CLOVA X. [Online]. Available: https://clova-x.naver.com/. [Accessed: Aug. 19, 2025].\n",
    "\n",
    "[7] Cohere, Cohere Chat. [Online]. Available: https://cohere.com/chat. [Accessed: Aug. 19, 2025].\n",
    "\n",
    "[8] Microsoft, GitHub Copilot. [Online]. Available: https://github.com/features/copilot. [Accessed: Aug. 19, 2025].\n",
    "\n",
    "[9] DeepSeek, DeepSeek Chat. [Online]. Available: https://chat.deepseek.com/. [Accessed: Aug. 19, 2025].\n",
    "\n",
    "[10] Baidu, ERNIE Bot (ÊñáÂøÉ‰∏ÄË®Ä). [Online]. Available: https://yiyan.baidu.com/. [Accessed: Aug. 19, 2025].\n",
    "\n",
    "[11] Google, Gemini. [Online]. Available: https://gemini.google.com/. [Accessed: Aug. 19, 2025].\n",
    "\n",
    "[12] xAI, Grok. [Online]. Available: https://grok.x.ai/. [Accessed: Aug. 19, 2025].\n",
    "\n",
    "[13] Hugging Face, HuggingChat. [Online]. Available: https://huggingface.co/chat/. [Accessed: Aug. 19, 2025].\n",
    "\n",
    "[14] Moonshot AI, Kimi Chat. [Online]. Available: https://kimi.moonshot.cn/. [Accessed: Aug. 19, 2025].\n",
    "\n",
    "[15] Mistral AI, Le Chat. [Online]. Available: https://chat.mistral.ai/. [Accessed: Aug. 19, 2025].\n",
    "\n",
    "[16] Meta, Meta AI. [Online]. Available: https://meta.ai/. [Accessed: Aug. 19, 2025].\n",
    "\n",
    "[17] Microsoft, Microsoft Copilot. [Online]. Available: https://copilot.microsoft.com/. [Accessed: Aug. 19, 2025].\n",
    "\n",
    "[18] Perplexity AI, Perplexity. [Online]. Available: https://www.perplexity.ai/. [Accessed: Aug. 19, 2025].\n",
    "\n",
    "[19] Inflection AI, Pi. [Online]. Available: https://pi.ai/. [Accessed: Aug. 19, 2025].\n",
    "\n",
    "[20] Alibaba Cloud, Qwen Chat (ÈÄö‰πâÂçÉÈóÆ). [Online]. Available: https://qianwen.aliyun.com/. [Accessed: Aug. 19, 2025].\n",
    "\n",
    "[21] Luka, Inc., Replika. [Online]. Available: https://replika.ai/. [Accessed: Aug. 19, 2025].\n",
    "\n",
    "[22] SenseTime, SenseNova (ÂïÜÊ±§Êó•Êó•Êñ∞). [Online]. Available: https://www.sensetime.com/en/business-group-SenseNova. [Accessed: Aug. 19, 2025].\n",
    "\n",
    "[23] iFlytek, SparkDesk (ËÆØÈ£ûÊòüÁÅ´). [Online]. Available: https://xinghuo.xfyun.cn/. [Accessed: Aug. 19, 2025].\n",
    "\n",
    "[24] You.com, YouChat. [Online]. Available: https://you.com/chat. [Accessed: Aug. 19,\n",
    "\n",
    "[25] OpenAI, ‚ÄúHello GPT-4o,‚Äù OpenAI Blog, May 13, 2024. [Online]. Available: https://openai.com/index/hello-gpt-4o/.\n",
    "\n",
    "[26] Gemini Team, ‚ÄúGemini 1.5: Unlocking multimodal understanding across hundreds of thousands of tokens,‚Äù Google for Developers, Feb. 15, 2024. [Online]. Available: https://developers.googleblog.com/2024/02/gemini-15-available-for-private-preview-in-google-ai-studio.html.\n",
    "\n",
    "[27] Anthropic, ‚ÄúIntroducing Claude 3.5 Sonnet,‚Äù Anthropic Research, Jun. 20, 2024. [Online]. Available: https://www.anthropic.com/news/claude-3-5-sonnet.\n",
    "\n",
    "[28] Meta AI, ‚ÄúIntroducing Meta Llama 3.1,‚Äù Meta AI, Jul. 23, 2024. [Online]. Available: https://ai.meta.com/blog/meta-llama-3-1/.\n",
    "\n",
    "[29] Cohere, ‚ÄúIntroducing Command R+: A Scalable LLM for Enterprises,‚Äù Cohere Blog, Apr. 4, 2024. [Online]. Available: https://txt.cohere.com/command-r-plus-scalable-llm-for-enterprises/.\n",
    "\n",
    "[30] Mistral AI Team, ‚ÄúAu Large: Our new flagship model, a new generative endpoint, and more,‚Äù Mistral AI, Jul. 11, 2024. [Online]. Available: https://mistral.ai/news/au-large/.\n",
    "\n",
    "[31] xAI, ‚ÄúAnnouncing Grok-2,‚Äù xAI Blog, Aug. 2024.\n",
    "\n",
    "[32] DeepSeek-AI, ‚ÄúDeepSeek-V3: A Strong, Economical, and Efficient Mixture-of-Experts Language Model,‚Äù arXiv preprint arXiv:2405.04434, 2024.\n",
    "\n",
    "[33] J. Qwen Team et al., ‚ÄúQwen2: A Family of Strong and General Open-Source Language Models,‚Äù arXiv preprint arXiv:2406.04832, 2024.\n",
    "\n",
    "[34] Moonshot AI, \"Kimi K2,\" Moonshot AI Research, Aug. 2024.\n",
    "\n",
    "[35] O. Sharir, Y. Levine, and A. Shashua, ‚ÄúThe Jurassic-2 series of language models,‚Äù AI21 Labs, Mar. 2023. [Online]. Available: https://www.ai21.com/blog/jurassic-2-language-models.\n",
    "\n",
    "[36] S. Vasisht, ‚ÄúHarness the power of Amazon Titan multimodal embeddings model in Amazon Bedrock,‚Äù AWS Machine Learning Blog, Apr. 2, 2024. [Online]. Available: https://aws.amazon.com/blogs/machine-learning/harness-the-power-of-amazon-titan-multimodal-embeddings-model-in-amazon-bedrock/.\n",
    "\n",
    "[37] Baidu, ‚ÄúERNIE 4.0: The latest foundation model from Baidu,‚Äù Baidu Research, Oct. 17, 2023.\n",
    "\n",
    "[38] M. A. Abdelfattah et al., ‚ÄúPhi-3: Redefining What‚Äôs Possible with Open Small Language Models,‚Äù Microsoft Research, 2024. [Online]. Available: https://www.microsoft.com/en-us/research/publication/phi-3-redefining-whats-possible-with-open-small-language-models/.\n",
    "\n",
    "[39] Technology Innovation Institute, ‚ÄúFalcon 180B: an open-source powerhouse,‚Äù Falcon LLM, Sep. 6, 2023. [Online]. Available: https://falconllm.tii.ae/falcon-180b.html.\n",
    "\n",
    "[40] 01.AI, ‚ÄúYi-1.5: A New Generation of Open-Source Models, Excelling in Code and Math,‚Äù 01.AI Blog, Jun. 5, 2024. [Online]. Available: https://www.01.ai/blog/yi-1.5.\n",
    "\n",
    "[41] Z. Zeng et al., ‚ÄúGLM-4: A new generation of multimodal language models,‚Äù Zhipu AI, 2024.\n",
    "\n",
    "[42] S. Black et al., ‚ÄúGPT-NeoX-20B: An Open-Source Autoregressive Language Model,‚Äù in Proceedings of the ACL Workshop on Challenges & Perspectives in Creating Large Language Models, 2022.\n",
    "\n",
    "[43] Project Gutenberg, \"Alice's Adventures in Wonderland, by Lewis Carroll,\" Project Gutenberg, 2008. [Online]. Available: https://www.gutenberg.org/ebooks/11. [Accessed: Aug. 19, 2025].\n",
    "\n",
    "[44] L. Carroll, Alice's Adventures in Wonderland. London, U.K.: Macmillan, 1865."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbfaf0b7-9fd0-4e81-a9cd-b35e929d2a20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
